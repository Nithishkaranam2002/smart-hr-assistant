{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63287575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "892909c6-742d-4395-bcf7-5d5d43f07534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Executive in the Sales department, aged 34, earns $4599 per month. They travel Travel_Rarely and have a job satisfaction score of 2. Their work-life balance rating is 4. This employee is not at risk of leaving. \n",
      "\n",
      "Sales Representative in the Sales department, aged 35, earns $2404 per month. They travel Travel_Rarely and have a job satisfaction score of 3. Their work-life balance rating is 3. This employee is not at risk of leaving. \n",
      "\n",
      "Laboratory Technician in the Research & Development department, aged 24, earns $3172 per month. They travel Travel_Frequently and have a job satisfaction score of 1. Their work-life balance rating is 2. This employee is at risk of leaving. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import and load dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file (adjust the path if needed)\n",
    "df = pd.read_csv(\"test.csv.xls\")\n",
    "\n",
    "# Step 2: Clean and drop unnecessary columns\n",
    "columns_to_drop = [\"EmployeeNumber\", \"StandardHours\", \"Over18\", \"EmployeeCount\"]\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Step 3: Generate a synthetic Attrition column\n",
    "df[\"Attrition\"] = np.where(\n",
    "    (df[\"WorkLifeBalance\"] <= 2) & (df[\"MonthlyIncome\"] < 4000),\n",
    "    \"Yes\",\n",
    "    \"No\"\n",
    ")\n",
    "\n",
    "# Step 4: Convert 'Attrition' to descriptive text\n",
    "df[\"Attrition\"] = df[\"Attrition\"].map({\n",
    "    \"Yes\": \"at risk of leaving\",\n",
    "    \"No\": \"not at risk of leaving\"\n",
    "})\n",
    "\n",
    "# Step 5: Define function to convert a row to a paragraph of text\n",
    "def row_to_paragraph(row):\n",
    "    return (\n",
    "        f\"{row['JobRole']} in the {row['Department']} department, aged {row['Age']}, \"\n",
    "        f\"earns ${row['MonthlyIncome']} per month. They travel {row['BusinessTravel']} and \"\n",
    "        f\"have a job satisfaction score of {row['JobSatisfaction']}. \"\n",
    "        f\"Their work-life balance rating is {row['WorkLifeBalance']}. \"\n",
    "        f\"This employee is {row['Attrition']}.\"\n",
    "    )\n",
    "\n",
    "# Step 6: Convert all rows to paragraphs\n",
    "employee_paragraphs = df.apply(row_to_paragraph, axis=1).tolist()\n",
    "\n",
    "# Step 7: Save as text file for RAG ingestion (optional)\n",
    "with open(\"employee_profiles.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for para in employee_paragraphs:\n",
    "        f.write(para + \"\\n\\n\")\n",
    "\n",
    "# Step 8: Preview the first few\n",
    "for p in employee_paragraphs[:3]:\n",
    "    print(p, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a8a2b7c-d67e-4b22-ac23-cd73d55d3354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/yd/0pl19lzn5n73hgzb9sf2b7v40000gn/T/ipykernel_56293/2099463759.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Employee summaries embedded and stored in ChromaDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yd/0pl19lzn5n73hgzb9sf2b7v40000gn/T/ipykernel_56293/2099463759.py:14: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embedding model\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Prepare the documents\n",
    "documents = employee_paragraphs  # from previous step\n",
    "\n",
    "# Create the Chroma vector store\n",
    "vectorstore = Chroma.from_texts(texts=documents, embedding=embedding_model, persist_directory=\"chroma_store\")\n",
    "vectorstore.persist()\n",
    "\n",
    "print(\" Employee summaries embedded and stored in ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b6c736-5d96-456b-8dae-e1976b815888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (0.24.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from groq) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from groq) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a309a5f5-79a3-4d82-87b9-e000a7d5ff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yd/0pl19lzn5n73hgzb9sf2b7v40000gn/T/ipykernel_56293/964314169.py:23: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HR Assistant Answer:\n",
      " Based on the provided context, here is a summary of employees with poor work-life balance (rating 2 or below) and low job satisfaction (rating 3 or below):\n",
      "\n",
      "* 1 employee in Research & Development, aged 42, with a work-life balance rating of 2 and a job satisfaction score of 3.\n",
      "* 1 employee in Research & Development, aged 39, with a work-life balance rating of 2 and a job satisfaction score of 4 (job satisfaction is not low, so this employee doesn't fully meet the criteria).\n",
      "* 1 employee in Research & Development, aged 43, with a work-life balance rating of 3 and a job satisfaction score of 3 (work-life balance is not poor, so this employee doesn't fully meet the criteria).\n",
      "* 1 employee in Research & Development, aged 42, is the best match.\n",
      "\n",
      "So, at least 1 employee (Research & Development, aged 42) has a poor work-life balance and low job satisfaction.\n",
      "\n",
      " Retrieved Sources:\n",
      "\n",
      " Source 1:\n",
      "Manager in the Research & Development department, aged 42, earns $18880 per month. They travel Travel_Rarely and have a job satisfaction score of 3. Their work-life balance rating is 2. This employee is not at risk of leaving.\n",
      "\n",
      " Source 2:\n",
      "Manager in the Human Resources department, aged 47, earns $19658 per month. They travel Travel_Rarely and have a job satisfaction score of 3. Their work-life balance rating is 3. This employee is not at risk of leaving.\n",
      "\n",
      " Source 3:\n",
      "Manager in the Research & Development department, aged 39, earns $19431 per month. They travel Non-Travel and have a job satisfaction score of 4. Their work-life balance rating is 2. This employee is not at risk of leaving.\n",
      "\n",
      " Source 4:\n",
      "Manager in the Research & Development department, aged 43, earns $19392 per month. They travel Travel_Rarely and have a job satisfaction score of 3. Their work-life balance rating is 3. This employee is not at risk of leaving.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set your Groq API key\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"  \n",
    "\n",
    "# Step 2: Import Groq's LangChain wrapper\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Step 3: Load the LLM from Groq using llama-4-scout\n",
    "llm = ChatGroq(\n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Step 4: Reload the Chroma vector DB from Phase 1\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load embedding model again (used during initial chunking)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Reload ChromaDB\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"chroma_store\",\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "# Step 5: Build RAG pipeline using Groq + Chroma\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Step 6: Ask your HR Assistant a question\n",
    "query = \"Summarize employees with poor work-life balance and low job satisfaction\"\n",
    "\n",
    "\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Step 7: Print the main answer\n",
    "print(\" HR Assistant Answer:\\n\", response['result'])\n",
    "\n",
    "# Optional: Show the source documents used\n",
    "print(\"\\n Retrieved Sources:\")\n",
    "for i, doc in enumerate(response['source_documents']):\n",
    "    print(f\"\\n Source {i+1}:\\n{doc.page_content}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e435a4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 824 employee summaries.\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "with open(\"/Users/nithishkaranam/Documents/smart-hr-assistant/notebooks/employee_profiles.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "docs = [Document(page_content=line.strip()) for line in lines]\n",
    "print(f\"✅ Loaded {len(docs)} employee summaries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a143e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nithishkaranam/Documents/smart-hr-assistant/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c646f939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split into 412 chunks.\n",
      "✅ Vector DB successfully saved to chroma_store/\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Step 1: Split docs into chunks\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"✅ Split into {len(chunks)} chunks.\")\n",
    "\n",
    "# Step 2: Load embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 3: Embed + save to ChromaDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"../chroma_store\"\n",
    ")\n",
    "vectorstore.persist()\n",
    "\n",
    "print(\"✅ Vector DB successfully saved to chroma_store/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
